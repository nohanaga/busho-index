{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Document Intelligence を使用した Semantic Chunker\n",
    "\n",
    "[Azure AI Document Intelligence](https://azure.microsoft.com/products/ai-services/ai-document-intelligence) は LangChain とネイティブに統合され、データの取り込み処理において精度を向上させることができます。\n",
    "\n",
    "まず、`AzureAIDocumentIntelligenceLoader` を使用して文書を **Markdown** 形式で読み取ります。その後、`MarkdownHeaderTextSplitter` が意味の理解に基づいてテキストをチャンクに分割します。こうすることで各チャンク内で意味の一貫性が維持されるという明確な利点があります。\n",
    "\n",
    "https://learn.microsoft.com/azure/ai-services/document-intelligence/concept-retrieval-augumented-generation?view=doc-intel-4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-documentintelligence langchain langchain-community tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.ai.documentintelligence\n",
    "print(\"Azure Document Intelligence version: \", azure.ai.documentintelligence.__version__)\n",
    "import langchain\n",
    "print(\"Langchain version: \", langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SDK targeting 2023-10-31-preview, make sure your resource is in one of these regions: East US, West US2, West Europe\n",
    "# pip install azure-ai-documentintelligence==1.0.0b1\n",
    "# pip install langchain langchain-community azure-ai-documentintelligence\n",
    "\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "\n",
    "endpoint = \"<your-endpoint>\"\n",
    "key = \"<your-key>\"\n",
    "\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    " \n",
    "# Azure AI Document Intelligence を起動して、ドキュメントを読み込みます。ドキュメントを読み込むには、file_path または url_path を指定します。\n",
    "loader = AzureAIDocumentIntelligenceLoader(file_path=\"./pdf/源実朝 - Wikipedia.pdf\", api_key = key, api_endpoint = endpoint, api_model=\"prebuilt-layout\", mode=\"markdown\")\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown ファイルとして書き出し\n",
    "ページコンテンツをそのまま Markdown ファイルとして書き出します。VSCode があれば、`.md` ファイルのプレビューがリアルタイムでできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('page_content.md', 'w') as f:\n",
    "    f.write(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MarkdownHeaderTextSplitter\n",
    "チャンク化戦略において、文書自体の構造を特に重視する場合、文書をマークダウン化し、特定のヘッダーグループごとにチャンクを作成することは直感的です。`MarkdownHeaderTextSplitter` は指定されたヘッダーのセットによってマークダウン ファイルが分割されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マークダウンのヘッダーに基づき、ドキュメントをチャンクに分割する。\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers = False)\n",
    " \n",
    "docs_string = docs[0].page_content\n",
    "splits = text_splitter.split_text(docs_string)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メタデータのみを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(splits):\n",
    "    print(i, text.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ドキュメントのトークン数を計測\n",
    "OpenAI モデルのトークン数を正確に数えるためには、[tiktoken](https://github.com/openai/tiktoken) ライブラリを利用します。モデルごとに利用するトークナイザーが異なるため詳しくは[こちら](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#チャンクごとのトークン数確認\n",
    "tokencounter = 0\n",
    "for i, text in enumerate(splits):\n",
    "    tokenlength = len(enc.encode(text.page_content))\n",
    "    print(i, tokenlength)\n",
    "    tokencounter = tokencounter + tokenlength\n",
    "\n",
    "print(tokencounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分割の確認\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 表形式の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RecursiveCharacterTextSplitter\n",
    "各マークダウン グループ内で、必要なテキスト スプリッターを適用できます。\n",
    "`RecursiveCharacterTextSplitter` は、一般的なテキストに推奨されます。文字のリストによってパラメータ化されます。チャンクが十分に小さくなるまで、順番に分割しようとします。デフォルトのリストは [\"\\n\\n\", \"\\n\", \" \", \"\"] です。これには、すべての段落 (次に文、そして単語) をできるだけ長くまとめて保持しようとする効果があります。これらの段落は、一般的に意味的に関連性が最も強いテキストであると思われるためです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 100\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits_2nd = text_splitter.split_documents(splits)\n",
    "splits_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#チャンクごとのトークン数確認\n",
    "tokencounter = 0\n",
    "for i, text in enumerate(splits_2nd):\n",
    "    tokenlength = len(enc.encode(text.page_content))\n",
    "    print(i, tokenlength)\n",
    "    tokencounter = tokencounter + tokenlength\n",
    "\n",
    "print(tokencounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_2nd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_2nd[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py0115",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
